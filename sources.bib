@article{akshathaHumanDetectionAerial2022,
  title = {Human {{Detection}} in {{Aerial Thermal Images Using Faster R-CNN}} and {{SSD Algorithms}}},
  author = {Akshatha, K. R. and Karunakar, A. Kotegar and Shenoy, Satish B. and Pai, Abhilash K. and Nagaraj, Nikhil Hunjanal and Rohatgi, Sambhav Singh},
  year = {2022},
  month = jan,
  journal = {Electronics},
  volume = {11},
  number = {7},
  pages = {1151},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics11071151},
  urldate = {2025-07-29},
  abstract = {The automatic detection of humans in aerial thermal imagery plays a significant role in various real-time applications, such as surveillance, search and rescue and border monitoring. Small target size, low resolution, occlusion, pose, and scale variations are the significant challenges in aerial thermal images that cause poor performance for various state-of-the-art object detection algorithms. Though many deep-learning-based object detection algorithms have shown impressive performance for generic object detection tasks, their ability to detect smaller objects in the aerial thermal images is analyzed through this study. This work carried out the performance evaluation of Faster R-CNN and single-shot multi-box detector (SSD) algorithms with different backbone networks to detect human targets in aerial view thermal images. For this purpose, two standard aerial thermal datasets having human objects of varying scale are considered with different backbone networks, such as ResNet50, Inception-v2, and MobileNet-v1. The evaluation results demonstrate that the Faster R-CNN model trained with the ResNet50 network architecture out-performed in terms of detection accuracy, with a mean average precision (mAP at 0.5 IoU) of 100\% and 55.7\% for the test data of the OSU thermal dataset and AAU PD T datasets, respectively. SSD with MobileNet-v1 achieved the highest detection speed of 44 frames per second (FPS) on the NVIDIA GeForce GTX 1080 GPU. Fine-tuning the anchor parameters of the Faster R-CNN ResNet50 and SSD Inception-v2 algorithms caused remarkable improvement in mAP by 10\% and 3.5\%, respectively, for the challenging AAU PD T dataset. The experimental results demonstrated the application of Faster R-CNN and SSD algorithms for human detection in aerial view thermal images, and the impact of varying backbone network and anchor parameters on the performance improvement of these algorithms.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {aerial images,convolutional neural network,Faster RCNN,human detection,object detection,SSD,thermal camera},
  file = {/Users/lukas/Zotero/storage/5AEQKNYM/Akshatha et al. - 2022 - Human Detection in Aerial Thermal Images Using Faster R-CNN and SSD Algorithms.pdf}
}

@misc{alqahtaniBenchmarkingDeepLearning2024,
  title = {Benchmarking {{Deep Learning Models}} for {{Object Detection}} on {{Edge Computing Devices}}},
  author = {Alqahtani, Daghash K. and Cheema, Aamir and Toosi, Adel N.},
  year = {2024},
  month = sep,
  number = {arXiv:2409.16808},
  eprint = {2409.16808},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.16808},
  urldate = {2025-08-13},
  abstract = {Modern applications, such as autonomous vehicles, require deploying deep learning algorithms on resource-constrained edge devices for real-time image and video processing. However, there is limited understanding of the efficiency and performance of various object detection models on these devices. In this paper, we evaluate state-of-the-art object detection models, including YOLOv8 (Nano, Small, Medium), EfficientDet Lite (Lite0, Lite1, Lite2), and SSD (SSD MobileNet V1, SSDLite MobileDet). We deployed these models on popular edge devices like the Raspberry Pi 3, 4, and 5 with/without TPU accelerators, and Jetson Orin Nano, collecting key performance metrics such as energy consumption, inference time, and Mean Average Precision (mAP). Our findings highlight that lower mAP models such as SSD MobileNet V1 are more energy-efficient and faster in inference, whereas higher mAP models like YOLOv8 Medium generally consume more energy and have slower inference, though with exceptions when accelerators like TPUs are used. Among the edge devices, Jetson Orin Nano stands out as the fastest and most energy-efficient option for request handling, despite having the highest idle energy consumption. These results emphasize the need to balance accuracy, speed, and energy efficiency when deploying deep learning models on edge devices, offering valuable guidance for practitioners and researchers selecting models and devices for their applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Hardware Architecture,Computer Science - Software Engineering},
  file = {/Users/lukas/Zotero/storage/LJDTB96N/Alqahtani et al. - 2024 - Benchmarking Deep Learning Models for Object Detection on Edge Computing Devices.pdf;/Users/lukas/Zotero/storage/SRXB2BWI/2409.html}
}

@inproceedings{beyererCNNbasedThermalInfrared2018,
  title = {{{CNN-based}} Thermal Infrared Person Detection by Domain Adaptation},
  booktitle = {Autonomous {{Systems}}: {{Sensors}}, {{Vehicles}}, {{Security}}, and the {{Internet}} of {{Everything}}},
  author = {Beyerer, J{\"u}rgen and Ruf, Miriam and Herrmann, Christian},
  editor = {Dudzik, Michael C. and Ricklin, Jennifer C.},
  year = {2018},
  month = may,
  pages = {8},
  publisher = {SPIE},
  address = {Orlando, United States},
  doi = {10.1117/12.2304400},
  urldate = {2025-08-12},
  abstract = {Imaging sensors capturing the surroundings of an autonomous vehicle are vital for its understanding of the environment. While thermal infrared cameras promise improved bad weather and nighttime robustness compared with standard RGBcameras, detecting objects, such as persons, in thermal infrared imagery is a tough problem because image resolution and quality is typically far lower, especially for low-cost sensors. Currently, deep learning based object detection frameworks offer an impressive performance on high-quality images. However, applying them to low-quality data in a different spectral range causes significant performance drops. This work proposes a strategy to make use of elaborate CNN-based object detector frameworks which are pre-trained on visual RGB images. Two key steps are undertaken: First, an appropriate preprocessing strategy for the IR data is suggested which transforms the IR data as close as possible to the RGB domain. This allows pre-trained RGB features to be effective on the novel domain. Second, the remaining domain gap is addressed by fine-tuning the pre-trained CNN on a limited set of thermal IR data. Different IR preprocessing options are explored, each addressing a different aspect of the domain gap between thermal IR and RGB data. Examples include dynamic range, blur or contrast. Because no preprocessing can cover all aspects alone, providing preprocessing combinations to the CNN allows addressing more than one aspect at once and further improves the results. Experiments indicate significant person detection improvements on the public KAIST dataset with the optimized preprocessing strategy.},
  isbn = {978-1-5106-1797-1 978-1-5106-1798-8},
  langid = {english},
  file = {/Users/lukas/Zotero/storage/D2XA5X3L/Beyerer et al. - 2018 - CNN-based thermal infrared person detection by domain adaptation.pdf}
}

@misc{Claude,
  title = {Claude},
  urldate = {2025-08-14},
  abstract = {Talk with Claude, an AI assistant from Anthropic},
  howpublished = {https://claude.ai/chat/1156c5b8-056c-48a6-a615-6b605f091938},
  langid = {american},
  file = {/Users/lukas/Zotero/storage/6SW2DP7J/1156c5b8-056c-48a6-a615-6b605f091938.html}
}

@article{cortesSupportvectorNetworks1995,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  year = {1995},
  month = sep,
  journal = {Machine Learning},
  volume = {20},
  number = {3},
  pages = {273--297},
  issn = {1573-0565},
  doi = {10.1007/BF00994018},
  urldate = {2025-08-12},
  abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  langid = {english},
  keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
  file = {/Users/lukas/Zotero/storage/R8KFXDXX/Cortes and Vapnik - 1995 - Support-vector networks.pdf}
}

@inproceedings{dalalHistogramsOrientedGradients2005,
  title = {Histograms of {{Oriented Gradients}} for {{Human Detection}}},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Dalal, N. and Triggs, B.},
  year = {2005},
  volume = {1},
  pages = {886--893},
  publisher = {IEEE},
  address = {San Diego, CA, USA},
  doi = {10.1109/CVPR.2005.177},
  urldate = {2025-08-12},
  abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  isbn = {978-0-7695-2372-9},
  langid = {english},
  file = {/Users/lukas/Zotero/storage/QY8RQJT7/Dalal and Triggs - 2005 - Histograms of Oriented Gradients for Human Detection.pdf}
}

@inproceedings{davisTwoStageTemplateApproach2005,
  title = {A {{Two-Stage Template Approach}} to {{Person Detection}} in {{Thermal Imagery}}},
  booktitle = {2005 {{Seventh IEEE Workshops}} on {{Applications}} of {{Computer Vision}} ({{WACV}}/{{MOTION}}'05) - {{Volume}} 1},
  author = {Davis, James W. and Keck, Mark A.},
  year = {2005},
  month = jan,
  volume = {1},
  pages = {364--369},
  doi = {10.1109/ACVMOT.2005.14},
  urldate = {2025-08-12},
  abstract = {We present a two-stage template-based method to detect people in widely varying thermal imagery. The approach initially performs a fast screening procedure using a generalized template to locate potential person locations. Next an AdaBoosted ensemble classifier using automatically tuned filters is employed to test the hypothesized person locations. We demonstrate and evaluate the approach using a challenging dataset of thermal imagery},
  keywords = {Automatic testing,Cameras,Computer science,Computer vision,Filter bank,Humans,Pixel,Shape,Thermal engineering,Video surveillance},
  file = {/Users/lukas/Zotero/storage/AEUHIRTF/4129504.html}
}

@inproceedings{davisTwoStageTemplateApproach2005a,
  title = {A {{Two-Stage Template Approach}} to {{Person Detection}} in {{Thermal Imagery}}},
  booktitle = {2005 {{Seventh IEEE Workshops}} on {{Applications}} of {{Computer Vision}} ({{WACV}}/{{MOTION}}'05) - {{Volume}} 1},
  author = {Davis, J.W. and Keck, M.A.},
  year = {2005},
  month = jan,
  pages = {364--369},
  publisher = {IEEE},
  address = {Breckenridge, CO},
  doi = {10.1109/ACVMOT.2005.14},
  urldate = {2025-08-12},
  abstract = {We present a two-stage template-based method to detect people in widely varying thermal imagery. The approach initially performs a fast screening procedure using a generalized template to locate potential person locations. Next an AdaBoosted ensemble classifier using automatically tuned filters is employed to test the hypothesized person locations. We demonstrate and evaluate the approach using a challenging dataset of thermal imagery.},
  isbn = {978-0-7695-2271-5},
  langid = {english},
  file = {/Users/lukas/Zotero/storage/T35VWLF3/Davis and Keck - 2005 - A Two-Stage Template Approach to Person Detection in Thermal Imagery.pdf}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2025-08-13},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
  file = {/Users/lukas/Zotero/storage/JT5NHHRZ/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;/Users/lukas/Zotero/storage/B35J7PNG/2010.html}
}

@misc{farooqObjectDetectionThermal2021,
  title = {Object {{Detection}} in {{Thermal Spectrum}} for {{Advanced Driver-Assistance Systems}} ({{ADAS}})},
  author = {Farooq, Muhammad Ali and Corcoran, Peter and Rotariu, Cosmin and Shariff, Waseem},
  year = {2021},
  month = oct,
  number = {arXiv:2109.09854},
  eprint = {2109.09854},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.09854},
  urldate = {2025-07-29},
  abstract = {Object detection in thermal infrared spectrum provides more reliable data source in low-lighting conditions and different weather conditions, as it is useful both in-cabin and outside for pedestrian, animal, and vehicular detection as well as for detecting street-signs \& lighting poles. This paper is about exploring and adapting state-of-the-art object detection and classifier framework on thermal vision with seven distinct classes for advanced driver-assistance systems (ADAS). The trained network variants on public datasets are validated on test data with three different test approaches which include test-time with no augmentation, test-time augmentation, and test-time with model ensembling. Additionally, the efficacy of trained networks is tested on locally gathered novel test-data captured with an uncooled LWIR prototype thermal camera in challenging weather and environmental scenarios. The performance analysis of trained models is investigated by computing precision, recall, and mean average precision scores (mAP). Furthermore, the trained model architecture is optimized using TensorRT inference accelerator and deployed on resource-constrained edge hardware Nvidia Jetson Nano to explicitly reduce the inference time on GPU as well as edge devices for further real-time onboard installations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: This work is carried under EU funded project (https://www.heliaus.eu/)},
  file = {/Users/lukas/Zotero/storage/9N4V7C9D/Farooq et al. - 2021 - Object Detection in Thermal Spectrum for Advanced Driver-Assistance Systems (ADAS).pdf;/Users/lukas/Zotero/storage/IW4UM65G/2109.html}
}

@misc{FREEFLIRThermal,
  title = {{{FREE}} - {{FLIR Thermal Dataset}} for {{Algorithm Training}} {\textbar} {{OEM}}.{{FLIR}}.Com},
  urldate = {2025-08-14},
  howpublished = {https://oem.flir.com/en-in/solutions/automotive/adas-dataset-form/},
  file = {/Users/lukas/Zotero/storage/4847QU9Z/adas-dataset-form.html}
}

@article{fukushimaNeocognitronSelforganizingNeural1980,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  shorttitle = {Neocognitron},
  author = {Fukushima, Kunihiko},
  year = {1980},
  month = apr,
  journal = {Biological Cybernetics},
  volume = {36},
  number = {4},
  pages = {193--202},
  issn = {1432-0770},
  doi = {10.1007/BF00344251},
  urldate = {2025-08-13},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by ``learning without a teacher'', and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname ``neocognitron''. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of ``S-cells'', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of ``C-cells'' similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any ``teacher'' during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  langid = {english},
  keywords = {Complex Cell,Digital Computer,Input Layer,Neural Network Model,Pattern Recognition},
  file = {/Users/lukas/Zotero/storage/HY4BAPCG/Fukushima - 1980 - Neocognitron A self-organizing neural network model for a mechanism of pattern recognition unaffect.pdf}
}

@misc{girshickFastRCNN2015,
  title = {Fast {{R-CNN}}},
  author = {Girshick, Ross},
  year = {2015},
  month = sep,
  number = {arXiv:1504.08083},
  eprint = {1504.08083},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1504.08083},
  urldate = {2025-08-18},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: To appear in ICCV 2015},
  file = {/Users/lukas/Zotero/storage/M5XUKT6A/Girshick - 2015 - Fast R-CNN.pdf;/Users/lukas/Zotero/storage/BM9B8TI7/1504.html}
}

@misc{girshickRichFeatureHierarchies2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  month = oct,
  number = {arXiv:1311.2524},
  eprint = {1311.2524},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1311.2524},
  urldate = {2025-08-18},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
  file = {/Users/lukas/Zotero/storage/M66MBJFE/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf;/Users/lukas/Zotero/storage/52TCKDE2/1311.html}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.03385},
  urldate = {2025-08-12},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Tech report},
  file = {/Users/lukas/Zotero/storage/QI2L2QUP/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/Users/lukas/Zotero/storage/KLBKCGNX/1512.html}
}

@article{hudaEffectDiverseDataset2020,
  title = {The {{Effect}} of a {{Diverse Dataset}} for {{Transfer Learning}} in {{Thermal Person Detection}}},
  author = {Huda, Noor Ul and Hansen, Bolette D. and Gade, Rikke and Moeslund, Thomas B.},
  year = {2020},
  month = jan,
  journal = {Sensors},
  volume = {20},
  number = {7},
  pages = {1982},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s20071982},
  urldate = {2025-07-29},
  abstract = {Thermal cameras are popular in detection for their precision in surveillance in the dark and for privacy preservation. In the era of data driven problem solving approaches, manually finding and annotating a large amount of data is inefficient in terms of cost and effort. With the introduction of transfer learning, rather than having large datasets, a dataset covering all characteristics and aspects of the target place is more important. In this work, we studied a large thermal dataset recorded for 20 weeks and identified nine phenomena in it. Moreover, we investigated the impact of each phenomenon for model adaptation in transfer learning. Each phenomenon was investigated separately and in combination. the performance was analyzed by computing the F1 score, precision, recall, true negative rate, and false negative rate. Furthermore, to underline our investigation, the trained model with our dataset was further tested on publicly available datasets, and encouraging results were obtained. Finally, our dataset was also made publicly available.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {CNN,databases,dataset,deep learning,detection,images,model adaptation,outdoor,person,thermal},
  file = {/Users/lukas/Zotero/storage/2FVF44KL/Huda et al. - 2020 - The Effect of a Diverse Dataset for Transfer Learning in Thermal Person Detection.pdf}
}

@inproceedings{hwangMultispectralPedestrianDetection2015,
  title = {Multispectral Pedestrian Detection: {{Benchmark}} Dataset and Baseline},
  shorttitle = {Multispectral Pedestrian Detection},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hwang, Soonmin and Park, Jaesik and Kim, Namil and Choi, Yukyung and Kweon, In So},
  year = {2015},
  month = jun,
  pages = {1037--1045},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298706},
  urldate = {2025-08-14},
  abstract = {With the increasing interest in pedestrian detection, pedestrian datasets have also been the subject of research in the past decades. However, most existing datasets focus on a color channel, while a thermal channel is helpful for detection even in a dark environment. With this in mind, we propose a multispectral pedestrian dataset which provides well aligned color-thermal image pairs, captured by beam splitter-based special hardware. The color-thermal dataset is as large as previous color-based datasets and provides dense annotations including temporal correspondences. With this dataset, we introduce multispectral ACF, which is an extension of aggregated channel features (ACF) to simultaneously handle color-thermal image pairs. Multispectral ACF reduces the average miss rate of ACF by 15\%, and achieves another breakthrough in the pedestrian detection task.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {/Users/lukas/Zotero/storage/BC7EQK33/Hwang et al. - 2015 - Multispectral pedestrian detection Benchmark dataset and baseline.pdf}
}

@misc{KOSTENLOSFLIRWarmebilddatensatzFur,
  title = {{{KOSTENLOS}} -- {{FLIR-W{\"a}rmebilddatensatz}} F{\"u}r Das {{Algorithmustraining}} {\textbar} {{OEM}}.{{FLIR}}.Com},
  urldate = {2025-08-14},
  howpublished = {https://oem.flir.com/de-de/solutions/automotive/adas-dataset-form/\#anchor1},
  file = {/Users/lukas/Zotero/storage/9XLHHSXS/adas-dataset-form.html}
}

@inproceedings{lecunHandwrittenDigitRecognition1989,
  title = {Handwritten {{Digit Recognition}} with a {{Back-Propagation Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
  year = {1989},
  volume = {2},
  publisher = {Morgan-Kaufmann},
  urldate = {2025-08-13},
  abstract = {We present an application of back-propagation networks to hand(cid:173) written digit recognition. Minimal preprocessing of the data was  required, but architecture of the network was highly constrained  and specifically designed for the task. The input of the network  consists of normalized images of isolated digits. The method has  1 \% error rate and about a 9\% reject rate on zipcode digits provided  by the U.S. Postal Service.},
  file = {/Users/lukas/Zotero/storage/F3IHILBD/LeCun et al. - 1989 - Handwritten Digit Recognition with a Back-Propagation Network.pdf}
}

@misc{linMicrosoftCOCOCommon2015,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll{\'a}r, Piotr},
  year = {2015},
  month = feb,
  number = {arXiv:1405.0312},
  eprint = {1405.0312},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1405.0312},
  urldate = {2025-08-18},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 1) updated annotation pipeline description and figures; 2) added new section describing datasets splits; 3) updated author list},
  file = {/Users/lukas/Zotero/storage/FZYHBI6T/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf;/Users/lukas/Zotero/storage/TW73PQ93/1405.html}
}

@incollection{liuSSDSingleShot2016,
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  shorttitle = {{{SSD}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  year = {2016},
  volume = {9905},
  eprint = {1512.02325},
  primaryclass = {cs},
  pages = {21--37},
  doi = {10.1007/978-3-319-46448-0_2},
  urldate = {2025-08-18},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300{\textbackslash}times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500{\textbackslash}times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: ECCV 2016},
  file = {/Users/lukas/Zotero/storage/Y8ITRXW9/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf;/Users/lukas/Zotero/storage/33U7VNDZ/1512.html}
}

@misc{liuTargetawareDualAdversarial2022,
  title = {Target-Aware {{Dual Adversarial Learning}} and a {{Multi-scenario Multi-Modality Benchmark}} to {{Fuse Infrared}} and {{Visible}} for {{Object Detection}}},
  author = {Liu, Jinyuan and Fan, Xin and Huang, Zhanbo and Wu, Guanyao and Liu, Risheng and Zhong, Wei and Luo, Zhongxuan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.16220},
  eprint = {2203.16220},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.16220},
  urldate = {2025-08-14},
  abstract = {This study addresses the issue of fusing infrared and visible images that appear differently for object detection. Aiming at generating an image of high visual quality, previous approaches discover commons underlying the two modalities and fuse upon the common space either by iterative optimization or deep networks. These approaches neglect that modality differences implying the complementary information are extremely important for both fusion and subsequent detection task. This paper proposes a bilevel optimization formulation for the joint problem of fusion and detection, and then unrolls to a target-aware Dual Adversarial Learning (TarDAL) network for fusion and a commonly used detection network. The fusion network with one generator and dual discriminators seeks commons while learning from differences, which preserves structural information of targets from the infrared and textural details from the visible. Furthermore, we build a synchronized imaging system with calibrated infrared and optical sensors, and collect currently the most comprehensive benchmark covering a wide range of scenarios. Extensive experiments on several public datasets and our benchmark demonstrate that our method outputs not only visually appealing fusion but also higher detection mAP than the state-of-the-art approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. (Oral)},
  file = {/Users/lukas/Zotero/storage/AX24K4Y8/Liu et al. - 2022 - Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrare.pdf;/Users/lukas/Zotero/storage/EWKP37PP/2203.html}
}

@misc{munirSSTNSelfSupervisedDomain2021,
  title = {{{SSTN}}: {{Self-Supervised Domain Adaptation Thermal Object Detection}} for {{Autonomous Driving}}},
  shorttitle = {{{SSTN}}},
  author = {Munir, Farzeen and Azam, Shoaib and Jeon, Moongu},
  year = {2021},
  month = nov,
  number = {arXiv:2103.03150},
  eprint = {2103.03150},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.03150},
  urldate = {2025-07-29},
  abstract = {The sensibility and sensitivity of the environment play a decisive role in the safe and secure operation of autonomous vehicles. This perception of the surrounding is way similar to human visual representation. The human's brain perceives the environment by utilizing different sensory channels and develop a view-invariant representation model. Keeping in this context, different exteroceptive sensors are deployed on the autonomous vehicle for perceiving the environment. The most common exteroceptive sensors are camera, Lidar and radar for autonomous vehicle's perception. Despite being these sensors have illustrated their benefit in the visible spectrum domain yet in the adverse weather conditions, for instance, at night, they have limited operation capability, which may lead to fatal accidents. In this work, we explore thermal object detection to model a view-invariant model representation by employing the self-supervised contrastive learning approach. For this purpose, we have proposed a deep neural network Self Supervised Thermal Network (SSTN) for learning the feature embedding to maximize the information between visible and infrared spectrum domain by contrastive learning, and later employing these learned feature representation for the thermal object detection using multi-scale encoder-decoder transformer network. The proposed method is extensively evaluated on the two publicly available datasets: the FLIR-ADAS dataset and the KAIST Multi-Spectral dataset. The experimental results illustrate the efficacy of the proposed method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/lukas/Zotero/storage/MBDTUK77/Munir et al. - 2021 - SSTN Self-Supervised Domain Adaptation Thermal Object Detection for Autonomous Driving.pdf;/Users/lukas/Zotero/storage/9L6C5WVG/2103.html}
}

@misc{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = may,
  number = {arXiv:1506.02640},
  eprint = {1506.02640},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02640},
  urldate = {2025-08-18},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/lukas/Zotero/storage/D449RTYY/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf;/Users/lukas/Zotero/storage/R4KUBDUL/1506.html}
}

@misc{renFasterRCNNRealTime2016,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2016},
  month = jan,
  number = {arXiv:1506.01497},
  eprint = {1506.01497},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.01497},
  urldate = {2025-08-18},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Extended tech report},
  file = {/Users/lukas/Zotero/storage/MJNCNMG7/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf;/Users/lukas/Zotero/storage/DCWMXZRR/1506.html}
}

@misc{ruderOverviewGradientDescent2017,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  year = {2017},
  month = jun,
  number = {arXiv:1609.04747},
  eprint = {1609.04747},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.04747},
  urldate = {2025-07-29},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Gradient Descent},
  note = {Comment: Added derivations of AdaMax and Nadam},
  file = {/Users/lukas/Zotero/storage/S3YP6J7P/Ruder - 2017 - An overview of gradient descent optimization algorithms.pdf;/Users/lukas/Zotero/storage/K8VX76QV/1609.html}
}

@misc{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  number = {arXiv:1409.1556},
  eprint = {1409.1556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.1556},
  urldate = {2025-08-18},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/lukas/Zotero/storage/AJ542YFS/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/Users/lukas/Zotero/storage/NMKPB7QF/1409.html}
}

@article{tsaiUsingDeepLearning2022,
  title = {Using {{Deep Learning}} with {{Thermal Imaging}} for {{Human Detection}} in {{Heavy Smoke Scenarios}}},
  author = {Tsai, Pei-Fen and Liao, Chia-Hung and Yuan, Shyan-Ming},
  year = {2022},
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {14},
  pages = {5351},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s22145351},
  urldate = {2025-08-12},
  abstract = {In this study, we propose using a thermal imaging camera (TIC) with a deep learning model as an intelligent human detection approach during emergency evacuations in a low-visibility smoky fire scenarios. We use low-wavelength infrared (LWIR) images taken by a TIC qualified with the National Fire Protection Association (NFPA) 1801 standards as input to the YOLOv4 model for real-time object detection. The model trained with a single Nvidia GeForce 2070 can achieve {$>$}95\% precision for the location of people in a low-visibility smoky scenario with 30.1 frames per second (FPS). This real-time result can be reported to control centers as useful information to help provide timely rescue and provide protection to firefighters before entering dangerous smoky fire situations.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {convolutional neural network,evacuation in fire,firefighter protection,human detection,human rescue,infrared thermal camera,LWIR,real-time object detection,smoky fire scene,thermal imaging camera,YOLO},
  file = {/Users/lukas/Zotero/storage/5R8BXD4N/Tsai et al. - 2022 - Using Deep Learning with Thermal Imaging for Human Detection in Heavy Smoke Scenarios.pdf}
}

@inproceedings{violaRapidObjectDetection2001,
  title = {Rapid Object Detection Using a Boosted Cascade of Simple Features},
  booktitle = {Proceedings of the 2001 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}. {{CVPR}} 2001},
  author = {Viola, P. and Jones, M.},
  year = {2001},
  volume = {1},
  pages = {I-511-I-518},
  publisher = {IEEE Comput. Soc},
  address = {Kauai, HI, USA},
  doi = {10.1109/CVPR.2001.990517},
  urldate = {2025-08-12},
  abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the ``Integral Image'' which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers[6]. The third contribution is a method for combining increasingly more complex classifiers in a ``cascade'' which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
  isbn = {978-0-7695-1272-3},
  langid = {english}
}
