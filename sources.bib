@article{akshathaHumanDetectionAerial2022,
  title = {Human {{Detection}} in {{Aerial Thermal Images Using Faster R-CNN}} and {{SSD Algorithms}}},
  author = {Akshatha, K. R. and Karunakar, A. Kotegar and Shenoy, Satish B. and Pai, Abhilash K. and Nagaraj, Nikhil Hunjanal and Rohatgi, Sambhav Singh},
  year = {2022},
  month = jan,
  journal = {Electronics},
  volume = {11},
  number = {7},
  pages = {1151},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics11071151},
  urldate = {2025-07-29},
  abstract = {The automatic detection of humans in aerial thermal imagery plays a significant role in various real-time applications, such as surveillance, search and rescue and border monitoring. Small target size, low resolution, occlusion, pose, and scale variations are the significant challenges in aerial thermal images that cause poor performance for various state-of-the-art object detection algorithms. Though many deep-learning-based object detection algorithms have shown impressive performance for generic object detection tasks, their ability to detect smaller objects in the aerial thermal images is analyzed through this study. This work carried out the performance evaluation of Faster R-CNN and single-shot multi-box detector (SSD) algorithms with different backbone networks to detect human targets in aerial view thermal images. For this purpose, two standard aerial thermal datasets having human objects of varying scale are considered with different backbone networks, such as ResNet50, Inception-v2, and MobileNet-v1. The evaluation results demonstrate that the Faster R-CNN model trained with the ResNet50 network architecture out-performed in terms of detection accuracy, with a mean average precision (mAP at 0.5 IoU) of 100\% and 55.7\% for the test data of the OSU thermal dataset and AAU PD T datasets, respectively. SSD with MobileNet-v1 achieved the highest detection speed of 44 frames per second (FPS) on the NVIDIA GeForce GTX 1080 GPU. Fine-tuning the anchor parameters of the Faster R-CNN ResNet50 and SSD Inception-v2 algorithms caused remarkable improvement in mAP by 10\% and 3.5\%, respectively, for the challenging AAU PD T dataset. The experimental results demonstrated the application of Faster R-CNN and SSD algorithms for human detection in aerial view thermal images, and the impact of varying backbone network and anchor parameters on the performance improvement of these algorithms.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {aerial images,convolutional neural network,Faster RCNN,human detection,object detection,SSD,thermal camera},
  file = {/Users/lukas/Zotero/storage/5AEQKNYM/Akshatha et al. - 2022 - Human Detection in Aerial Thermal Images Using Faster R-CNN and SSD Algorithms.pdf}
}

@misc{alqahtaniBenchmarkingDeepLearning2024,
  title = {Benchmarking {{Deep Learning Models}} for {{Object Detection}} on {{Edge Computing Devices}}},
  author = {Alqahtani, Daghash K. and Cheema, Aamir and Toosi, Adel N.},
  year = {2024},
  month = sep,
  number = {arXiv:2409.16808},
  eprint = {2409.16808},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.16808},
  urldate = {2025-08-13},
  abstract = {Modern applications, such as autonomous vehicles, require deploying deep learning algorithms on resource-constrained edge devices for real-time image and video processing. However, there is limited understanding of the efficiency and performance of various object detection models on these devices. In this paper, we evaluate state-of-the-art object detection models, including YOLOv8 (Nano, Small, Medium), EfficientDet Lite (Lite0, Lite1, Lite2), and SSD (SSD MobileNet V1, SSDLite MobileDet). We deployed these models on popular edge devices like the Raspberry Pi 3, 4, and 5 with/without TPU accelerators, and Jetson Orin Nano, collecting key performance metrics such as energy consumption, inference time, and Mean Average Precision (mAP). Our findings highlight that lower mAP models such as SSD MobileNet V1 are more energy-efficient and faster in inference, whereas higher mAP models like YOLOv8 Medium generally consume more energy and have slower inference, though with exceptions when accelerators like TPUs are used. Among the edge devices, Jetson Orin Nano stands out as the fastest and most energy-efficient option for request handling, despite having the highest idle energy consumption. These results emphasize the need to balance accuracy, speed, and energy efficiency when deploying deep learning models on edge devices, offering valuable guidance for practitioners and researchers selecting models and devices for their applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Hardware Architecture,Computer Science - Software Engineering},
  file = {/Users/lukas/Zotero/storage/LJDTB96N/Alqahtani et al. - 2024 - Benchmarking Deep Learning Models for Object Detection on Edge Computing Devices.pdf;/Users/lukas/Zotero/storage/SRXB2BWI/2409.html}
}

@inproceedings{beyererCNNbasedThermalInfrared2018,
  title = {{{CNN-based}} Thermal Infrared Person Detection by Domain Adaptation},
  booktitle = {Autonomous {{Systems}}: {{Sensors}}, {{Vehicles}}, {{Security}}, and the {{Internet}} of {{Everything}}},
  author = {Beyerer, J{\"u}rgen and Ruf, Miriam and Herrmann, Christian},
  editor = {Dudzik, Michael C. and Ricklin, Jennifer C.},
  year = {2018},
  month = may,
  pages = {8},
  publisher = {SPIE},
  address = {Orlando, United States},
  doi = {10.1117/12.2304400},
  urldate = {2025-08-12},
  abstract = {Imaging sensors capturing the surroundings of an autonomous vehicle are vital for its understanding of the environment. While thermal infrared cameras promise improved bad weather and nighttime robustness compared with standard RGBcameras, detecting objects, such as persons, in thermal infrared imagery is a tough problem because image resolution and quality is typically far lower, especially for low-cost sensors. Currently, deep learning based object detection frameworks offer an impressive performance on high-quality images. However, applying them to low-quality data in a different spectral range causes significant performance drops. This work proposes a strategy to make use of elaborate CNN-based object detector frameworks which are pre-trained on visual RGB images. Two key steps are undertaken: First, an appropriate preprocessing strategy for the IR data is suggested which transforms the IR data as close as possible to the RGB domain. This allows pre-trained RGB features to be effective on the novel domain. Second, the remaining domain gap is addressed by fine-tuning the pre-trained CNN on a limited set of thermal IR data. Different IR preprocessing options are explored, each addressing a different aspect of the domain gap between thermal IR and RGB data. Examples include dynamic range, blur or contrast. Because no preprocessing can cover all aspects alone, providing preprocessing combinations to the CNN allows addressing more than one aspect at once and further improves the results. Experiments indicate significant person detection improvements on the public KAIST dataset with the optimized preprocessing strategy.},
  isbn = {978-1-5106-1797-1 978-1-5106-1798-8},
  langid = {english},
  file = {/Users/lukas/Zotero/storage/D2XA5X3L/Beyerer et al. - 2018 - CNN-based thermal infrared person detection by domain adaptation.pdf}
}

@inproceedings{bridleTrainingStochasticModel1989,
  title = {Training {{Stochastic Model Recognition Algorithms}} as {{Networks}} Can {{Lead}} to {{Maximum Mutual Information Estimation}} of {{Parameters}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bridle, John},
  year = {1989},
  volume = {2},
  publisher = {Morgan-Kaufmann},
  urldate = {2025-08-19},
  abstract = {One  of the  attractions  of neural  network  approaches  to  pattern  recognition  is  the  use  of a  discrimination-based  training  method.  We  show  that once  we  have  modified  the  output  layer  of a  multi(cid:173) layer perceptron to provide mathematically correct  probability dis(cid:173) tributions,  and  replaced  the  usual  squared  error  criterion  with  a  probability-based score,  the  result  is equivalent  to  Maximum  Mu(cid:173) tual  Information training,  which  has  been  used  successfully  to im(cid:173) prove  the  performance  of hidden  Markov models for  speech  recog(cid:173) nition.  If the network is specially constructed to perform the recog(cid:173) nition computations of a  given kind of stochastic model based clas(cid:173) sifier  then we  obtain a  method for  discrimination-based training of  the  parameters  of the  models.  Examples  include  an  HMM-based  word discriminator,  which  we  call an 'Alphanet'.},
  file = {/Users/lukas/Zotero/storage/Q3CC46VK/Bridle - 1989 - Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information.pdf}
}

@article{burnhamComparisonRobertsSobel1997,
  title = {Comparison of the {{Roberts}}, {{Sobel}}, {{Robinson}}, {{Canny}}, and {{Hough Image Detection Algorithms}}},
  author = {Burnham, John and Hardy, Jonathan and Meadors, Kyle},
  year = {1997},
  abstract = {This paper presents a comparison and evaluation of the Roberts, Sobel, Robinson, Canny, and Hough image detection algorithms based on their ability to detect red squares on a black and white background. Inspired by the Institute of Electrical and Electronics Engineers (IEEE) Southeastcon student hardware competition, the algorithms are tested on an image database comprised of gray scale images taken from a test platform containing red squares on a black and white background. The success of each algorithm is based on its accuracy in detecting the red squares within the images from the database. The results of the algorithms are then compared statistically to determine which one is the best suited for this application.},
  langid = {english},
  file = {/Users/lukas/Zotero/storage/BWT2D4X2/Burnham et al. - Comparison of the Roberts, Sobel, Robinson, Canny, and Hough Image Detection Algorithms.pdf}
}

@article{cortesSupportvectorNetworks1995,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  year = {1995},
  month = sep,
  journal = {Machine Learning},
  volume = {20},
  number = {3},
  pages = {273--297},
  issn = {1573-0565},
  doi = {10.1007/BF00994018},
  urldate = {2025-08-12},
  abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  langid = {english},
  keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
  file = {/Users/lukas/Zotero/storage/R8KFXDXX/Cortes and Vapnik - 1995 - Support-vector networks.pdf}
}

@misc{costaFurtherGeneralizationsJaccard2021,
  title = {Further {{Generalizations}} of the {{Jaccard Index}}},
  author = {Costa, Luciano da F.},
  year = {2021},
  month = nov,
  number = {arXiv:2110.09619},
  eprint = {2110.09619},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.09619},
  urldate = {2025-08-20},
  abstract = {Quantifying the similarity between two mathematical structures or datasets constitutes a particularly interesting and useful operation in several theoretical and applied problems. Aimed at this specific objective, the Jaccard index has been extensively used in the most diverse types of problems, also motivating some respective generalizations. The present work addresses further generalizations of this index, including its modification into a coincidence index capable of accounting also for the level of relative interiority between the two compared entities, as well as respective extensions for sets in continuous vector spaces, the generalization to multiset addition, densities and generic scalar fields, as well as a means to quantify the joint interdependence between two random variables. The also interesting possibility to take into account more than two sets has also been addressed, including the description of an index capable of quantifying the level of chaining between three structures. Several of the described and suggested eneralizations have been illustrated with respect to numeric case examples. It is also posited that these indices can play an important role while analyzing and integrating datasets in modeling approaches and pattern recognition activities, including as a measurement of clusters similarity or separation and as a resource for representing and analyzing complex networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: 15 pages, 13 figures, a preprint},
  file = {/Users/lukas/Zotero/storage/DG49A59H/Costa - 2021 - Further Generalizations of the Jaccard Index.pdf;/Users/lukas/Zotero/storage/R7BC97MU/2110.html}
}

@inproceedings{dalalHistogramsOrientedGradients2005,
  title = {Histograms of {{Oriented Gradients}} for {{Human Detection}}},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Dalal, N. and Triggs, B.},
  year = {2005},
  volume = {1},
  pages = {886--893},
  publisher = {IEEE},
  address = {San Diego, CA, USA},
  doi = {10.1109/CVPR.2005.177},
  urldate = {2025-08-12},
  abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  isbn = {978-0-7695-2372-9},
  langid = {english},
  file = {/Users/lukas/Zotero/storage/QY8RQJT7/Dalal and Triggs - 2005 - Histograms of Oriented Gradients for Human Detection.pdf}
}

@inproceedings{davisTwoStageTemplateApproach2005,
  title = {A {{Two-Stage Template Approach}} to {{Person Detection}} in {{Thermal Imagery}}},
  booktitle = {2005 {{Seventh IEEE Workshops}} on {{Applications}} of {{Computer Vision}} ({{WACV}}/{{MOTION}}'05) - {{Volume}} 1},
  author = {Davis, James W. and Keck, Mark A.},
  year = {2005},
  month = jan,
  volume = {1},
  pages = {364--369},
  doi = {10.1109/ACVMOT.2005.14},
  urldate = {2025-08-12},
  abstract = {We present a two-stage template-based method to detect people in widely varying thermal imagery. The approach initially performs a fast screening procedure using a generalized template to locate potential person locations. Next an AdaBoosted ensemble classifier using automatically tuned filters is employed to test the hypothesized person locations. We demonstrate and evaluate the approach using a challenging dataset of thermal imagery},
  keywords = {Automatic testing,Cameras,Computer science,Computer vision,Filter bank,Humans,Pixel,Shape,Thermal engineering,Video surveillance},
  file = {/Users/lukas/Zotero/storage/AEUHIRTF/4129504.html}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  month = jun,
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  urldate = {2025-09-11},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500--1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {/Users/lukas/Zotero/storage/KNX2YVH7/Deng et al. - ImageNet A Large-Scale Hierarchical Image Database.pdf;/Users/lukas/Zotero/storage/DF7TN2S9/5206848.html}
}

@misc{dengInadequatelyPretrainedModels2023,
  title = {Towards {{Inadequately Pre-trained Models}} in {{Transfer Learning}}},
  author = {Deng, Andong and Li, Xingjian and Hu, Di and Wang, Tianyang and Xiong, Haoyi and Xu, Chengzhong},
  year = {2023},
  month = aug,
  number = {arXiv:2203.04668},
  eprint = {2203.04668},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.04668},
  urldate = {2025-08-19},
  abstract = {Pre-training has been a popular learning paradigm in deep learning era, especially in annotation-insufficient scenario. Better ImageNet pre-trained models have been demonstrated, from the perspective of architecture, by previous research to have better transferability to downstream tasks. However, in this paper, we found that during the same pre-training process, models at middle epochs, which is inadequately pre-trained, can outperform fully trained models when used as feature extractors (FE), while the fine-tuning (FT) performance still grows with the source performance. This reveals that there is not a solid positive correlation between top-1 accuracy on ImageNet and the transferring result on target data. Based on the contradictory phenomenon between FE and FT that better feature extractor fails to be fine-tuned better accordingly, we conduct comprehensive analyses on features before softmax layer to provide insightful explanations. Our discoveries suggest that, during pre-training, models tend to first learn spectral components corresponding to large singular values and the residual components contribute more when fine-tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted by ICCV'2023},
  file = {/Users/lukas/Zotero/storage/ET5H5N8F/Deng et al. - 2023 - Towards Inadequately Pre-trained Models in Transfer Learning.pdf;/Users/lukas/Zotero/storage/FNMFU976/2203.html}
}

@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2010.11929},
  urldate = {2025-08-13},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  note = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
  file = {/Users/lukas/Zotero/storage/JT5NHHRZ/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;/Users/lukas/Zotero/storage/B35J7PNG/2010.html}
}

@misc{dubeyActivationFunctionsDeep2022,
  title = {Activation {{Functions}} in {{Deep Learning}}: {{A Comprehensive Survey}} and {{Benchmark}}},
  shorttitle = {Activation {{Functions}} in {{Deep Learning}}},
  author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
  year = {2022},
  month = jun,
  number = {arXiv:2109.14545},
  eprint = {2109.14545},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.14545},
  urldate = {2025-08-22},
  abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: {\textbackslash}url\{https://github.com/shivram1987/ActivationFunctions\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: Accepted in Neurocomputing, Elsevier},
  file = {/Users/lukas/Zotero/storage/AEWMGS5A/Dubey et al. - 2022 - Activation Functions in Deep Learning A Comprehensive Survey and Benchmark.pdf;/Users/lukas/Zotero/storage/CUW8RPRL/2109.html}
}

@misc{elharroussLossFunctionsDeep2025,
  title = {Loss {{Functions}} in {{Deep Learning}}: {{A Comprehensive Review}}},
  shorttitle = {Loss {{Functions}} in {{Deep Learning}}},
  author = {Elharrouss, Omar and Mahmood, Yasir and Bechqito, Yassine and Serhani, Mohamed Adel and Badidi, Elarbi and Riffi, Jamal and Tairi, Hamid},
  year = {2025},
  month = apr,
  number = {arXiv:2504.04242},
  eprint = {2504.04242},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.04242},
  urldate = {2025-08-20},
  abstract = {Loss functions are at the heart of deep learning, shaping how models learn and perform across diverse tasks. They are used to quantify the difference between predicted outputs and ground truth labels, guiding the optimization process to minimize errors. Selecting the right loss function is critical, as it directly impacts model convergence, generalization, and overall performance across various applications, from computer vision to time series forecasting. This paper presents a comprehensive review of loss functions, covering fundamental metrics like Mean Squared Error and Cross-Entropy to advanced functions such as Adversarial and Diffusion losses. We explore their mathematical foundations, impact on model training, and strategic selection for various applications, including computer vision (Discriminative and generative), tabular data prediction, and time series forecasting. For each of these categories, we discuss the most used loss functions in the recent advancements of deep learning techniques. Also, this review explore the historical evolution, computational efficiency, and ongoing challenges in loss function design, underlining the need for more adaptive and robust solutions. Emphasis is placed on complex scenarios involving multi-modal data, class imbalances, and real-world constraints. Finally, we identify key future directions, advocating for loss functions that enhance interpretability, scalability, and generalization, leading to more effective and resilient deep learning models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/lukas/Zotero/storage/4QMAWDH6/Elharrouss et al. - 2025 - Loss Functions in Deep Learning A Comprehensive Review.pdf;/Users/lukas/Zotero/storage/G5G53AYZ/2504.html}
}

@misc{erhanScalableObjectDetection2013,
  title = {Scalable {{Object Detection}} Using {{Deep Neural Networks}}},
  author = {Erhan, Dumitru and Szegedy, Christian and Toshev, Alexander and Anguelov, Dragomir},
  year = {2013},
  month = dec,
  number = {arXiv:1312.2249},
  eprint = {1312.2249},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.2249},
  urldate = {2025-08-20},
  abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {/Users/lukas/Zotero/storage/3Z2IBGRV/Erhan et al. - 2013 - Scalable Object Detection using Deep Neural Networks.pdf;/Users/lukas/Zotero/storage/G6XKWZE9/1312.html}
}

@misc{farooqObjectDetectionThermal2021,
  title = {Object {{Detection}} in {{Thermal Spectrum}} for {{Advanced Driver-Assistance Systems}} ({{ADAS}})},
  author = {Farooq, Muhammad Ali and Corcoran, Peter and Rotariu, Cosmin and Shariff, Waseem},
  year = {2021},
  month = oct,
  number = {arXiv:2109.09854},
  eprint = {2109.09854},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.09854},
  urldate = {2025-07-29},
  abstract = {Object detection in thermal infrared spectrum provides more reliable data source in low-lighting conditions and different weather conditions, as it is useful both in-cabin and outside for pedestrian, animal, and vehicular detection as well as for detecting street-signs \& lighting poles. This paper is about exploring and adapting state-of-the-art object detection and classifier framework on thermal vision with seven distinct classes for advanced driver-assistance systems (ADAS). The trained network variants on public datasets are validated on test data with three different test approaches which include test-time with no augmentation, test-time augmentation, and test-time with model ensembling. Additionally, the efficacy of trained networks is tested on locally gathered novel test-data captured with an uncooled LWIR prototype thermal camera in challenging weather and environmental scenarios. The performance analysis of trained models is investigated by computing precision, recall, and mean average precision scores (mAP). Furthermore, the trained model architecture is optimized using TensorRT inference accelerator and deployed on resource-constrained edge hardware Nvidia Jetson Nano to explicitly reduce the inference time on GPU as well as edge devices for further real-time onboard installations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: This work is carried under EU funded project (https://www.heliaus.eu/)},
  file = {/Users/lukas/Zotero/storage/9N4V7C9D/Farooq et al. - 2021 - Object Detection in Thermal Spectrum for Advanced Driver-Assistance Systems (ADAS).pdf;/Users/lukas/Zotero/storage/IW4UM65G/2109.html}
}

@misc{FREEFLIRThermal,
  title = {{{FREE}} - {{FLIR Thermal Dataset}} for {{Algorithm Training}} {\textbar} {{OEM}}.{{FLIR}}.Com},
  urldate = {2025-08-14},
  howpublished = {https://oem.flir.com/en-in/solutions/automotive/adas-dataset-form/},
  file = {/Users/lukas/Zotero/storage/4847QU9Z/adas-dataset-form.html}
}

@article{fukushimaNeocognitronSelforganizingNeural1980,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  shorttitle = {Neocognitron},
  author = {Fukushima, Kunihiko},
  year = {1980},
  month = apr,
  journal = {Biological Cybernetics},
  volume = {36},
  number = {4},
  pages = {193--202},
  issn = {1432-0770},
  doi = {10.1007/BF00344251},
  urldate = {2025-08-13},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by ``learning without a teacher'', and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname ``neocognitron''. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of ``S-cells'', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of ``C-cells'' similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any ``teacher'' during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  langid = {english},
  keywords = {Complex Cell,Digital Computer,Input Layer,Neural Network Model,Pattern Recognition},
  file = {/Users/lukas/Zotero/storage/HY4BAPCG/Fukushima - 1980 - Neocognitron A self-organizing neural network model for a mechanism of pattern recognition unaffect.pdf}
}

@misc{girshickFastRCNN2015,
  title = {Fast {{R-CNN}}},
  author = {Girshick, Ross},
  year = {2015},
  month = sep,
  number = {arXiv:1504.08083},
  eprint = {1504.08083},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1504.08083},
  urldate = {2025-08-18},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: To appear in ICCV 2015},
  file = {/Users/lukas/Zotero/storage/M5XUKT6A/Girshick - 2015 - Fast R-CNN.pdf;/Users/lukas/Zotero/storage/BM9B8TI7/1504.html}
}

@misc{girshickRichFeatureHierarchies2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  month = oct,
  number = {arXiv:1311.2524},
  eprint = {1311.2524},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1311.2524},
  urldate = {2025-08-18},
  abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
  file = {/Users/lukas/Zotero/storage/M66MBJFE/Girshick et al. - 2014 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf;/Users/lukas/Zotero/storage/52TCKDE2/1311.html}
}

@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1512.03385},
  urldate = {2025-08-12},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Tech report},
  file = {/Users/lukas/Zotero/storage/QI2L2QUP/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/Users/lukas/Zotero/storage/KLBKCGNX/1512.html}
}

@article{hudaEffectDiverseDataset2020,
  title = {The {{Effect}} of a {{Diverse Dataset}} for {{Transfer Learning}} in {{Thermal Person Detection}}},
  author = {Huda, Noor Ul and Hansen, Bolette D. and Gade, Rikke and Moeslund, Thomas B.},
  year = {2020},
  month = jan,
  journal = {Sensors},
  volume = {20},
  number = {7},
  pages = {1982},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s20071982},
  urldate = {2025-07-29},
  abstract = {Thermal cameras are popular in detection for their precision in surveillance in the dark and for privacy preservation. In the era of data driven problem solving approaches, manually finding and annotating a large amount of data is inefficient in terms of cost and effort. With the introduction of transfer learning, rather than having large datasets, a dataset covering all characteristics and aspects of the target place is more important. In this work, we studied a large thermal dataset recorded for 20 weeks and identified nine phenomena in it. Moreover, we investigated the impact of each phenomenon for model adaptation in transfer learning. Each phenomenon was investigated separately and in combination. the performance was analyzed by computing the F1 score, precision, recall, true negative rate, and false negative rate. Furthermore, to underline our investigation, the trained model with our dataset was further tested on publicly available datasets, and encouraging results were obtained. Finally, our dataset was also made publicly available.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {CNN,databases,dataset,deep learning,detection,images,model adaptation,outdoor,person,thermal},
  file = {/Users/lukas/Zotero/storage/2FVF44KL/Huda et al. - 2020 - The Effect of a Diverse Dataset for Transfer Learning in Thermal Person Detection.pdf}
}

@inproceedings{hwangMultispectralPedestrianDetection2015,
  title = {Multispectral Pedestrian Detection: {{Benchmark}} Dataset and Baseline},
  shorttitle = {Multispectral Pedestrian Detection},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Hwang, Soonmin and Park, Jaesik and Kim, Namil and Choi, Yukyung and Kweon, In So},
  year = {2015},
  month = jun,
  pages = {1037--1045},
  publisher = {IEEE},
  address = {Boston, MA, USA},
  doi = {10.1109/CVPR.2015.7298706},
  urldate = {2025-08-14},
  abstract = {With the increasing interest in pedestrian detection, pedestrian datasets have also been the subject of research in the past decades. However, most existing datasets focus on a color channel, while a thermal channel is helpful for detection even in a dark environment. With this in mind, we propose a multispectral pedestrian dataset which provides well aligned color-thermal image pairs, captured by beam splitter-based special hardware. The color-thermal dataset is as large as previous color-based datasets and provides dense annotations including temporal correspondences. With this dataset, we introduce multispectral ACF, which is an extension of aggregated channel features (ACF) to simultaneously handle color-thermal image pairs. Multispectral ACF reduces the average miss rate of ACF by 15\%, and achieves another breakthrough in the pedestrian detection task.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {/Users/lukas/Zotero/storage/BC7EQK33/Hwang et al. - 2015 - Multispectral pedestrian detection Benchmark dataset and baseline.pdf}
}

@misc{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = mar,
  number = {arXiv:1502.03167},
  eprint = {1502.03167},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.03167},
  urldate = {2025-08-20},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/lukas/Zotero/storage/JBBJE7TM/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf;/Users/lukas/Zotero/storage/NVG34UCX/1502.html}
}

@inproceedings{lecunHandwrittenDigitRecognition1989,
  title = {Handwritten {{Digit Recognition}} with a {{Back-Propagation Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
  year = {1989},
  volume = {2},
  publisher = {Morgan-Kaufmann},
  urldate = {2025-08-13},
  abstract = {We present an application of back-propagation networks to hand(cid:173) written digit recognition. Minimal preprocessing of the data was  required, but architecture of the network was highly constrained  and specifically designed for the task. The input of the network  consists of normalized images of isolated digits. The method has  1 \% error rate and about a 9\% reject rate on zipcode digits provided  by the U.S. Postal Service.},
  file = {/Users/lukas/Zotero/storage/F3IHILBD/LeCun et al. - 1989 - Handwritten Digit Recognition with a Back-Propagation Network.pdf}
}

@misc{linMicrosoftCOCOCommon2015,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll{\'a}r, Piotr},
  year = {2015},
  month = feb,
  number = {arXiv:1405.0312},
  eprint = {1405.0312},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1405.0312},
  urldate = {2025-08-18},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: 1) updated annotation pipeline description and figures; 2) added new section describing datasets splits; 3) updated author list},
  file = {/Users/lukas/Zotero/storage/FZYHBI6T/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf;/Users/lukas/Zotero/storage/TW73PQ93/1405.html}
}

@incollection{liuSSDSingleShot2016,
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  shorttitle = {{{SSD}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  year = {2016},
  volume = {9905},
  eprint = {1512.02325},
  primaryclass = {cs},
  pages = {21--37},
  doi = {10.1007/978-3-319-46448-0_2},
  urldate = {2025-08-20},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For \$300{\textbackslash}times 300\$ input, SSD achieves 72.1\% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for \$500{\textbackslash}times 500\$ input, SSD achieves 75.1\% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: ECCV 2016},
  file = {/Users/lukas/Zotero/storage/Q2NU8ED6/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf;/Users/lukas/Zotero/storage/26IK5AJS/1512.html}
}

@misc{liuTargetawareDualAdversarial2022,
  title = {Target-Aware {{Dual Adversarial Learning}} and a {{Multi-scenario Multi-Modality Benchmark}} to {{Fuse Infrared}} and {{Visible}} for {{Object Detection}}},
  author = {Liu, Jinyuan and Fan, Xin and Huang, Zhanbo and Wu, Guanyao and Liu, Risheng and Zhong, Wei and Luo, Zhongxuan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.16220},
  eprint = {2203.16220},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.16220},
  urldate = {2025-08-14},
  abstract = {This study addresses the issue of fusing infrared and visible images that appear differently for object detection. Aiming at generating an image of high visual quality, previous approaches discover commons underlying the two modalities and fuse upon the common space either by iterative optimization or deep networks. These approaches neglect that modality differences implying the complementary information are extremely important for both fusion and subsequent detection task. This paper proposes a bilevel optimization formulation for the joint problem of fusion and detection, and then unrolls to a target-aware Dual Adversarial Learning (TarDAL) network for fusion and a commonly used detection network. The fusion network with one generator and dual discriminators seeks commons while learning from differences, which preserves structural information of targets from the infrared and textural details from the visible. Furthermore, we build a synchronized imaging system with calibrated infrared and optical sensors, and collect currently the most comprehensive benchmark covering a wide range of scenarios. Extensive experiments on several public datasets and our benchmark demonstrate that our method outputs not only visually appealing fusion but also higher detection mAP than the state-of-the-art approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Accepted by IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. (Oral)},
  file = {/Users/lukas/Zotero/storage/AX24K4Y8/Liu et al. - 2022 - Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrare.pdf;/Users/lukas/Zotero/storage/EWKP37PP/2203.html}
}

@misc{micikeviciusMixedPrecisionTraining2018,
  title = {Mixed {{Precision Training}}},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  year = {2018},
  month = feb,
  number = {arXiv:1710.03740},
  eprint = {1710.03740},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1710.03740},
  urldate = {2025-09-11},
  abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published as a conference paper at ICLR 2018},
  file = {/Users/lukas/Zotero/storage/K2F9P5LL/Micikevicius et al. - 2018 - Mixed Precision Training.pdf;/Users/lukas/Zotero/storage/EC2EFCVU/1710.html}
}

@misc{munirSSTNSelfSupervisedDomain2021,
  title = {{{SSTN}}: {{Self-Supervised Domain Adaptation Thermal Object Detection}} for {{Autonomous Driving}}},
  shorttitle = {{{SSTN}}},
  author = {Munir, Farzeen and Azam, Shoaib and Jeon, Moongu},
  year = {2021},
  month = nov,
  number = {arXiv:2103.03150},
  eprint = {2103.03150},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.03150},
  urldate = {2025-07-29},
  abstract = {The sensibility and sensitivity of the environment play a decisive role in the safe and secure operation of autonomous vehicles. This perception of the surrounding is way similar to human visual representation. The human's brain perceives the environment by utilizing different sensory channels and develop a view-invariant representation model. Keeping in this context, different exteroceptive sensors are deployed on the autonomous vehicle for perceiving the environment. The most common exteroceptive sensors are camera, Lidar and radar for autonomous vehicle's perception. Despite being these sensors have illustrated their benefit in the visible spectrum domain yet in the adverse weather conditions, for instance, at night, they have limited operation capability, which may lead to fatal accidents. In this work, we explore thermal object detection to model a view-invariant model representation by employing the self-supervised contrastive learning approach. For this purpose, we have proposed a deep neural network Self Supervised Thermal Network (SSTN) for learning the feature embedding to maximize the information between visible and infrared spectrum domain by contrastive learning, and later employing these learned feature representation for the thermal object detection using multi-scale encoder-decoder transformer network. The proposed method is extensively evaluated on the two publicly available datasets: the FLIR-ADAS dataset and the KAIST Multi-Spectral dataset. The experimental results illustrate the efficacy of the proposed method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/lukas/Zotero/storage/MBDTUK77/Munir et al. - 2021 - SSTN Self-Supervised Domain Adaptation Thermal Object Detection for Autonomous Driving.pdf;/Users/lukas/Zotero/storage/9L6C5WVG/2103.html}
}

@article{PDFCustomObject2025,
  title = {({{PDF}}) {{Custom Object Detection Using Transfer Learning}} with {{Pretrained Models}} for {{Improved Detection Techniques}}},
  year = {2025},
  month = aug,
  journal = {ResearchGate},
  doi = {10.37899/journallamultiapp.v5i1.843},
  urldate = {2025-08-19},
  abstract = {PDF {\textbar} Custom object detection plays a vital role in computer vision applications. However, developing an accurate and efficient custom object detector... {\textbar} Find, read and cite all the research you need on ResearchGate},
  langid = {english},
  file = {/Users/lukas/Zotero/storage/L8IFAM9C/2025 - (PDF) Custom Object Detection Using Transfer Learning with Pretrained Models for Improved Detection.pdf;/Users/lukas/Zotero/storage/E3TCGKTI/377950093_Custom_Object_Detection_Using_Transfer_Learning_with_Pretrained_Models_for_Improved_D.html}
}

@misc{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real-Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = may,
  number = {arXiv:1506.02640},
  eprint = {1506.02640},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.02640},
  urldate = {2025-08-18},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/lukas/Zotero/storage/D449RTYY/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Detection.pdf;/Users/lukas/Zotero/storage/R4KUBDUL/1506.html}
}

@misc{renFasterRCNNRealTime2016,
  title = {Faster {{R-CNN}}: {{Towards Real-Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R-CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2016},
  month = jan,
  number = {arXiv:1506.01497},
  eprint = {1506.01497},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.01497},
  urldate = {2025-08-18},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Extended tech report},
  file = {/Users/lukas/Zotero/storage/MJNCNMG7/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf;/Users/lukas/Zotero/storage/DCWMXZRR/1506.html}
}

@misc{ResNetV15PyTorch,
  title = {{{ResNet}} v1.5 for {{PyTorch}} {\textbar} {{NVIDIA NGC}}},
  urldate = {2025-08-22},
  howpublished = {https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet\_50\_v1\_5\_for\_pytorch},
  file = {/Users/lukas/Zotero/storage/4LJX7H6K/resnet_50_v1_5_for_pytorch.html}
}

@misc{ruderOverviewGradientDescent2017,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  year = {2017},
  month = jun,
  number = {arXiv:1609.04747},
  eprint = {1609.04747},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.04747},
  urldate = {2025-07-29},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Gradient Descent},
  note = {Comment: Added derivations of AdaMax and Nadam},
  file = {/Users/lukas/Zotero/storage/S3YP6J7P/Ruder - 2017 - An overview of gradient descent optimization algorithms.pdf;/Users/lukas/Zotero/storage/K8VX76QV/1609.html}
}

@misc{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  number = {arXiv:1409.1556},
  eprint = {1409.1556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1409.1556},
  urldate = {2025-08-18},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/lukas/Zotero/storage/AJ542YFS/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/Users/lukas/Zotero/storage/NMKPB7QF/1409.html}
}

@inproceedings{teutschLowResolutionPerson2014,
  title = {Low {{Resolution Person Detection}} with a {{Moving Thermal Infrared Camera}} by {{Hot Spot Classification}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Teutsch, Michael and Mueller, Thomas and Huber, Marco and Beyerer, Juergen},
  year = {2014},
  month = jun,
  pages = {209--216},
  publisher = {IEEE},
  address = {Columbus, OH, USA},
  doi = {10.1109/CVPRW.2014.40},
  urldate = {2025-08-21},
  abstract = {In many visual surveillance applications the task of person detection and localization can be solved easier by using thermal long-wave infrared (LWIR) cameras which are less affected by changing illumination or background texture than visual-optical cameras. Especially in outdoor scenes where usually only few hot spots appear in thermal infrared imagery, humans can be detected more reliably due to their prominent infrared signature. We propose a two-stage person recognition approach for LWIR images: (1) the application of Maximally Stable Extremal Regions (MSER) to detect hot spots instead of background subtraction or sliding window and (2) the verification of the detected hot spots using a Discrete Cosine Transform (DCT) based descriptor and a modified Random Na{\textasciidieresis}{\i}ve Bayes (RNB) classifier. The main contributions are the novel modified RNB classifier and the generality of our method. We achieve high detection rates for several different LWIR datasets with low resolution videos in real-time. While many papers in this topic are dealing with strong constraints such as considering only one dataset, assuming a stationary camera, or detecting only moving persons, we aim at avoiding such constraints to make our approach applicable with moving platforms such as Unmanned Ground Vehicles (UGV).},
  isbn = {978-1-4799-4308-1},
  langid = {english},
  file = {/Users/lukas/Zotero/storage/YS3FABMC/Teutsch et al. - 2014 - Low Resolution Person Detection with a Moving Thermal Infrared Camera by Hot Spot Classification.pdf}
}

@article{tsaiUsingDeepLearning2022,
  title = {Using {{Deep Learning}} with {{Thermal Imaging}} for {{Human Detection}} in {{Heavy Smoke Scenarios}}},
  author = {Tsai, Pei-Fen and Liao, Chia-Hung and Yuan, Shyan-Ming},
  year = {2022},
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {14},
  pages = {5351},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s22145351},
  urldate = {2025-08-12},
  abstract = {In this study, we propose using a thermal imaging camera (TIC) with a deep learning model as an intelligent human detection approach during emergency evacuations in a low-visibility smoky fire scenarios. We use low-wavelength infrared (LWIR) images taken by a TIC qualified with the National Fire Protection Association (NFPA) 1801 standards as input to the YOLOv4 model for real-time object detection. The model trained with a single Nvidia GeForce 2070 can achieve {$>$}95\% precision for the location of people in a low-visibility smoky scenario with 30.1 frames per second (FPS). This real-time result can be reported to control centers as useful information to help provide timely rescue and provide protection to firefighters before entering dangerous smoky fire situations.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {convolutional neural network,evacuation in fire,firefighter protection,human detection,human rescue,infrared thermal camera,LWIR,real-time object detection,smoky fire scene,thermal imaging camera,YOLO},
  file = {/Users/lukas/Zotero/storage/5R8BXD4N/Tsai et al. - 2022 - Using Deep Learning with Thermal Imaging for Human Detection in Heavy Smoke Scenarios.pdf}
}

@inproceedings{violaRapidObjectDetection2001a,
  title = {Rapid Object Detection Using a Boosted Cascade of Simple Features},
  booktitle = {Proceedings of the 2001 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}. {{CVPR}} 2001},
  author = {Viola, P. and Jones, M.},
  year = {2001},
  volume = {1},
  pages = {I-511-I-518},
  publisher = {IEEE Comput. Soc},
  address = {Kauai, HI, USA},
  doi = {10.1109/CVPR.2001.990517},
  urldate = {2025-08-20},
  abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the ``Integral Image'' which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers[6]. The third contribution is a method for combining increasingly more complex classifiers in a ``cascade'' which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
  isbn = {978-0-7695-1272-3},
  langid = {english},
  file = {/Users/lukas/Zotero/storage/WXQA65Z2/Viola and Jones - 2001 - Rapid object detection using a boosted cascade of simple features.pdf}
}

@misc{zhuangComprehensiveSurveyTransfer2020,
  title = {A {{Comprehensive Survey}} on {{Transfer Learning}}},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  year = {2020},
  month = jun,
  number = {arXiv:1911.02685},
  eprint = {1911.02685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.02685},
  urldate = {2025-08-19},
  abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 31 pages, 7 figures},
  file = {/Users/lukas/Zotero/storage/355E5QIT/Zhuang et al. - 2020 - A Comprehensive Survey on Transfer Learning.pdf;/Users/lukas/Zotero/storage/HD7T6PTS/1911.html}
}
